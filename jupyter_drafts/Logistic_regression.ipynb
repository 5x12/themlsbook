{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links:\n",
    "- github sklearn https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_logistic.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL LOGIC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vot eto kak-to nado sdelat' ne ebu kak:\n",
    "\n",
    "- penalty \n",
    "- dual\n",
    "- tol \n",
    "- C \n",
    "- fit_intercept \n",
    "- intercept_scaling\n",
    "- class_weight\n",
    "- random_state\n",
    "- solver\n",
    "- max_iter\n",
    "- multi_class\n",
    "- verbose\n",
    "- warm_start\n",
    "- n_jobs\n",
    "- l1_ration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intercept\n",
    "\n",
    "The idea behind adding intercept is to account for the bias in the linear equation, which originally underlines logistic regression. If we do not add it, we are taking an assumption, that our probability of observing output equal to 1 is 0.5 when all the weights are 0. From mathematical point of view, we have an equation of the logistic regression model, which is:\n",
    "\n",
    "$ P(y=1|x) = sigmoid(w_0 + \\sum^n_{i=1} w_i x_i) $, where\n",
    "\n",
    "$ P(y=1|x) $ - probability of classifying our output as 1,\n",
    "\n",
    "$ \\sum^n_{i=1} w_i x_i $ - logit function and\n",
    "\n",
    "$ w_o $ - intercept term.\n",
    "\n",
    "\n",
    "To include the intercept to our model, we create an additional feature with constant value of ones and add it ot the input feature matrix. The resulting matrix will have one extra columns of ones, which then will multiply wirh $w_0$ and will allow us to account intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.hstack((intercept, X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "\n",
    "Firstly, we can dig into activation function of Logistic Regression. \n",
    "\n",
    "An activation function is a mathematical function that controls the output of regression. Activation is responsible for adding non-linearity to the output of a neural network model. Without an activation function, a neural network is simply a linear regression.\n",
    "\n",
    "Sigmoid is a mathematical function that takes any real number and maps it to a probability between 1 and 0. That idea lies behind the Logistic Regression, where we predict a probabilty of classifier to be either 1 or 0 based on the feature'  values and make a classification.\n",
    "\n",
    "$ \\text{Logit Function} = log(\\dfrac{p}{1-p}) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n $\n",
    "\n",
    "We can transform it to a matrix equation which will look like this:\n",
    "\n",
    "$ \\text{Logit Function} = log(\\dfrac{p}{1-p}) = W^T X $\n",
    "\n",
    "To calculate the probability we should express `p` variable from this equation:\n",
    "\n",
    "$ p = \\dfrac{1}{1 + e^{-W^T X}} $\n",
    "\n",
    "Now we can code this equation as a `sigmoid` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HIDE\n",
    "\n",
    "x = np.linspace(-10, 10, 50)   \n",
    "p = sigmoid(x)\n",
    "plt.xlabel('x') \n",
    "plt.ylabel('Sigmoid')  \n",
    "plt.plot(x, p) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cost function is needed to understand how bad our model is. It's needed to undertand, which features require more weights to have a better fit of our model and increase our estimation of Y based on X. \n",
    "\n",
    "In logistic regression we mainly use 2 different cost functions: `L2` (Ridge) which is default and `L1` (Lasso). The difference between them is the penalty term.\n",
    "\n",
    "`L2` regularization adds the squared error to our formula as a penaty and helps to avoid underfitting, while `L1` adds the absolute value makes coefficients with lower importance to zero and removing some features. \n",
    "\n",
    "The formula for it is:\n",
    "\n",
    "$ L_2 = \\sum^n_{i=1}  (y_i - \\sum^p_{j=1} x_{ij} \\beta_j)^2 + \\lambda \\sum^p_{j=1} \\beta^2_j $\n",
    "\n",
    "$ L_1 = \\sum^n_{i=1}  (y_i - \\sum^p_{j=1} x_{ij} \\beta_j)^2 + \\lambda \\sum^p_{j=1} | \\beta_j | $\n",
    "\n",
    "Knowing this, we can strat writing our function to define cost functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(self, w, X, y):\n",
    "    m = len(y)\n",
    "    h = self.sigmoid(np.dot(X, w))\n",
    "    if self.penalty == 'l1': # L1 regularization\n",
    "        reg_term = self.C * np.sum(np.abs(w))\n",
    "    elif self.penalty == 'l2': # L2 regularization\n",
    "        reg_term = 0.5 * self.C * np.sum(w**2)\n",
    "    else:  # None penalty term, i.e. OLS cost function\n",
    "        reg_term = 0\n",
    "    cost = -np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) / m + reg_term\n",
    "    return cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But one more thing we have is `elastic net`. The idea behind this is that we can mix both regularizations together to have a more advanced penalty term. By doing this we define $ l_1 \\text{ }ratio $ which controls the input of both `L1` and `L2` regularizations to our custome regularization by stating the relationship between them. So the final function will look as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(self, w, X, y):\n",
    "    m = len(y)\n",
    "    h = self.sigmoid(np.dot(X, w))\n",
    "    if self.penalty == 'l1': # L1 regularization\n",
    "        reg_term = self.C * np.sum(np.abs(w))\n",
    "    elif self.penalty == 'l2': # L2 regularization\n",
    "        reg_term = 0.5 * self.C * np.sum(w**2)\n",
    "    elif self.penalty == 'elasticnet': # Elastic net regularization\n",
    "        reg_term = self.C * (self.l1_ratio * np.sum(np.abs(w)) + 0.5 * (1 - self.l1_ratio) * np.sum(w**2))\n",
    "    else:  # None penalty term, i.e. OLS cost function\n",
    "        reg_term = 0\n",
    "    cost = -np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) / m + reg_term\n",
    "    return cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient and gradient descent\n",
    "\n",
    "Gradient is a critical component of in optimization process of a model. It is a vector of partial derivatives of a cost function with respect to the weights of the features. So basically gradient  shows the direction of the steepest increase of the function and indicates the rate of change in that direction. \n",
    "\n",
    "To calculate gradient of the function, we can use the formula for the vectorized form of it, which is:\n",
    "\n",
    "$ \\nabla J = \\dfrac{1}{m} \\cdot X^T \\cdot (h(X \\cdot w) - y) $, where\n",
    "\n",
    "$X$ - input matrix,\n",
    "\n",
    "$y$ - vector of actual class labels and\n",
    "\n",
    "$h(X \\cdot w)$ - vector of predicted probabilities\n",
    "\n",
    "Knowing this, we can write the first part of `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(self, w, X, y):\n",
    "        m = len(y)\n",
    "        h = self._sigmoid(np.dot(X, w))\n",
    "        gradient = np.dot(X.T, (h - y)) / m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to take into account what cost function do we use, because it will affect our regularizatiom term. Also, we need to take into account that we also have 1 more parameter of our model, which is C. C is a inverse regularization strength. It controls the balance of our model between under- and overfitting. The lower value of C we take, the stronger is our regularization, i.e. the more we prevent overfitting of our model by reducing its complexity. This parameter is being accounted by multiplying its value on regularization term for every cost function in gradient descent.\n",
    "\n",
    "The next step is to calculate the value of regularization term based on cost function we have chosen before. If we decided to use L1 (Lasso) regularization, the formula for it would be \n",
    "\n",
    "$ L_1 = C \\cdot \\sum |w_i| $\n",
    "\n",
    "and after calculating the gradient we will receive\n",
    "\n",
    "$ \\dfrac{\\partial L_1}{\\partial w_i} = C \\cdot sgn(w_i) $\n",
    "\n",
    "Here we use a signum of the $w_i$, which return 1 if the number is greater than 0, -1 if the number is lower and 0 if 0. That is because when we calculate $\\dfrac{\\partial |w_i|}{\\partial w_i}$ we get just the sign of the fraction.\n",
    "\n",
    "If the cost function we are using is L2 (Ridge), than our goal is to find the derivative of our regularization term\n",
    "\n",
    "$ L_2 = 0.5 \\cdot C \\cdot \\sum w_i^2 $\n",
    "\n",
    "that is \n",
    "\n",
    "$ \\dfrac{\\partial L_2}{\\partial w_i} = C \\cdot w_i $\n",
    "\n",
    "We are multiplying our regularization term by 0.5 here (and also while writing the cost function function) to simplify our gradient expression, as it will cancel our the multiplication by 2, which comes from taking the derivative of squared value of weights. \n",
    "\n",
    "Finally, for elasticnet it will just the combination of both regularization terms with respect to our l1 ratio.\n",
    "\n",
    "The complete code will result in this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(self, w, X, y):\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(np.dot(X, w))\n",
    "        gradient = np.dot(X.T, (h - y)) / m\n",
    "\n",
    "        if self.penalty == 'l1':\n",
    "            reg_term = self.C * np.sign(w)\n",
    "        elif self.penalty == 'l2':\n",
    "            reg_term = self.C * w\n",
    "        elif self.penalty == 'elasticnet':\n",
    "            reg_term = self.C * (self.l1_ratio * np.sign(w) + (1 - self.l1_ratio) * w)\n",
    "        else:  # none\n",
    "            reg_term = 0\n",
    "        return gradient + reg_term"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "\n",
    "Now we come to the main thing of building a regression: solver functions.\n",
    "\n",
    "Solver is needed to find the optimal weights for our model. Overall, there are 6 solvers the can possibly be applied to logistic regression: lbfgs, liblinear, newton-cg, sag and saga. The choice of the algorithm depends on the penalty chosen, because most of them does not support all the available cost functions.\n",
    "\n",
    "Let's analyze all of them one by one"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the gradient descent algorithm, we need to create a loop of finding gradient until we met the optimal solution. Also we can add learning rate there to control the adjustment of the weights to the gradient. The lower we set our learning rate, the more accurate our result would be, but also the more time and power will it take to calculate the final output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(self, X, y, w):\n",
    "        for _ in range(self.max_iter):\n",
    "            gradient = self.gradient(w, X, y)\n",
    "            w -= self.learning_rate * gradient\n",
    "        return w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAG\n",
    "\n",
    "Stochastic Average Gradient Descent (or SAG) is an optimization algorithm which iterates a random sample of the dataset to find the optimal state of the model. It is well suited for a large-scaled datasets and supports L2 regularization or None.\n",
    "\n",
    "SAG mainly have 2 steps:\n",
    "\n",
    "1. Calculate Stochastic Gradient Descent (SGD). In alternative to standart gradient descent, SGD uses randomly choosen point at each iteration to calculate the gradient descent. It brings more noise to the, but helps to reduce the time of calculation.\n",
    "2. Averaging the gradient values. To deal with the noise problem, SAG takes the average values of gradients and updates the parameter.\n",
    "\n",
    "Mathematically, we have and objective function as a sum of functions in every point devided by the number of points, i.e.\n",
    "\n",
    "$ L_w = \\dfrac{1}{m} \\cdot \\sum L_i(w) $\n",
    "\n",
    "We calculate the gradients for each $L_i$ and than update the value of average gradient\n",
    "\n",
    "$ G = G + \\dfrac{1}{m} (g_i(w) - g_i(w_{old})) $\n",
    "\n",
    "After this, we update the weights with respect to learning rate\n",
    "\n",
    "$ w = w - \\eta \\cdot G $\n",
    "\n",
    "But calculating gradients in points also applies some restrictions on the solver. For example, it cannot work with L1 and elastic net regularizations. Dut to the fact, that the regularization term in this models is the absolute value of weights, the function is not smooth, which can affect the result and they can vary a much based on random sampling results.\n",
    "\n",
    "To implement the logic of SAG in code, we need to create a storage for sum of gradients and also initialize a matrix to store values of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sag(self, X, y, w):\n",
    "    m, n = X.shape\n",
    "    sum_gradients = np.zeros_like(w)\n",
    "    gradients = np.zeros((m, n))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to make a stated in max_iter amount of iteration through all points. During each iteration we calculate new gradient for the point and add regularization term there. After this we calculate the add the difference between old and new gradient to the sum of gradients and update vector of weights using the average gradient multiplied by learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sag(self, X, y, w):\n",
    "    m, n = X.shape\n",
    "    sum_gradients = np.zeros_like(w)\n",
    "    gradients = np.zeros((m, n))\n",
    "\n",
    "    for epoch in range(self.max_iter):\n",
    "        for i in range(m):\n",
    "            old_gradient = gradients[i]\n",
    "            new_gradient = (self.sigmoid(np.dot(X[i], w)) - y[i]) * X[i]\n",
    "            if self.penalty == 'l2':\n",
    "                reg_term = self.C * w\n",
    "            else:  # none\n",
    "                reg_term = 0\n",
    "            new_gradient += reg_term\n",
    "            gradients[i] = new_gradient\n",
    "            sum_gradients += new_gradient - old_gradient\n",
    "            w -= self.learning_rate * sum_gradients / m\n",
    "    return w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAGA\n",
    "\n",
    "Stochastic Average Gradient with Asymptotic Variance Reduction (SAGA) is a solver that can deal with the problem of non-smoothness of L1 regularization in SAG. The rule of updating gradients here uses proximal step while updating weights. \n",
    "\n",
    "https://math.stackexchange.com/questions/1961888/the-proximal-operator-of-the-l-1-norm-function\n",
    "https://math.stackexchange.com/questions/1766811/l-1-regularized-unconstrained-optimization-problem\n",
    "\n",
    "The optimization problem here is:\n",
    "\n",
    "$ \\text{Prox}(w) = \\text{argmin}_u \\text{  }{ \\dfrac{1}{2} \\cdot ||w - u||^2 + \\lambda \\cdot ||u||} $\n",
    "\n",
    "We can understand, that $u$ here can be either positive, negative or zero (почему??? я понял основную логику, но почему берем сигнум не особо ).\n",
    "\n",
    "For $u>0$ the derivative vanishes to $u_i = w_i - \\lambda$ and holds for every $w_i>\\lambda$. We apply the same logic to derive $u_i = w_i + \\lambda$ for $u<0$. Finally, for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saga(self, X, y, w):\n",
    "    m, n = X.shape\n",
    "    sum_gradients = np.zeros_like(w)\n",
    "    gradients = np.zeros((m, n))\n",
    "    \n",
    "    for epoch in range(self.max_iter):\n",
    "        for i in range(m):\n",
    "            old_gradient = gradients[i]\n",
    "            new_gradient = (self.sigmoid(np.dot(X[i], w)) - y[i]) * X[i]\n",
    "            if self.penalty == 'l1':\n",
    "                reg_term = self.C * np.sign(w)\n",
    "            elif self.penalty == 'l2':\n",
    "                reg_term = self.C * w\n",
    "            elif self.penalty == 'elasticnet':\n",
    "                reg_term = self.C * (self.l1_ratio * np.sign(w) + (1 - self.l1_ratio) * w)\n",
    "            else:  # none\n",
    "                reg_term = 0\n",
    "            new_gradient += reg_term\n",
    "            gradients[i] = new_gradient\n",
    "            sum_gradients += new_gradient - old_gradient\n",
    "            w -= self.learning_rate * sum_gradients / m\n",
    "        if self.penalty == 'l1':\n",
    "            w = np.sign(w) * np.maximum(np.abs(w) - self.C * self.learning_rate, 0)\n",
    "    return w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton-CG\n",
    "\n",
    "Newton-Conjugate Gradient is a concept that combines both Newton's and Conjugate Gradient methods. \n",
    "\n",
    "The idea behind Newton's method is an optimization algorithm that uses Hessian matrix (the matrix of second-order derivatives) to optimize the function. It recalculates the values of minimum of the function by taking steps in the direction of inverse Hessian multiplied by gradient matrix.\n",
    "\n",
    "The scheme for this iterations is \n",
    "\n",
    "$ w_{i+1} = w_i - \\gamma [\\nabla^2 w_i]^T \\cdot \\nabla w_i $, where\n",
    "\n",
    "w - estimation for the vector of weights, \n",
    "\n",
    "$\\gamma$ - size of the step.\n",
    "\n",
    "Conjugate Gradient method (add math here)\n",
    "\n",
    "As said before, Newton-CG combines both this algorithms and calculates the product of Hessian and gradient matrixes. \n",
    "\n",
    "To implement this algorithm in code, we firstly need to define a function to calculate Hessian matrix.\n",
    "Here we firstly calculate sigmoid function and then calculate the Hessian. After this, we are adding regularization term for L2 using diagonal of the matrix, because it will help us to add squared sum of weights, as it should be in formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(self, w, X, y):\n",
    "    h = self.sigmoid(np.dot(X, w))\n",
    "    hessian = np.dot(((h * (1 - h)).reshape(-1, 1) * X).T, X)\n",
    "    \n",
    "    if self.penalty == 'l2':\n",
    "        reg_term = self.C * np.diag(w)\n",
    "    else:  # none\n",
    "        reg_term = 0\n",
    "    return hessian + reg_term"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to write code for the solver itself. For each iteration in max_iter values, we calculate gradient, hessian and then the step direction. The step direction is calculated as the product of these two matrixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_cg(self, X, y, w):\n",
    "    for _ in range(self.max_iter):\n",
    "        gradient = self.gradient(w, X, y)\n",
    "        hessian = self.hessian(w, X, y)\n",
    "        step_direction = np.linalg.solve(hessian, gradient)\n",
    "        w -= self.learning_rate * step_direction\n",
    "    return w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working for now\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, penalty='l2', tol=1e-4, C=1.0, fit_intercept=True,\n",
    "                 intercept_scaling=1, class_weight=None, random_state=None,\n",
    "                 solver='lbfgs', max_iter=100, warm_start=False, n_jobs=None,\n",
    "                 l1_ratio=None, learning_rate=0.1):\n",
    "        self.penalty = penalty\n",
    "        self.tol = tol\n",
    "        self.C = C\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.intercept_scaling = intercept_scaling\n",
    "        self.class_weight = class_weight\n",
    "        self.random_state = random_state\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "        self.warm_start = warm_start\n",
    "        self.n_jobs = n_jobs\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.hstack((intercept, X))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def cost_function(self, w, X, y):\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(np.dot(X, w))\n",
    "        if self.penalty == 'l1':\n",
    "            reg_term = self.C * np.sum(np.abs(w))\n",
    "        elif self.penalty == 'l2':\n",
    "            reg_term = 0.5 * self.C * np.sum(w**2)\n",
    "        elif self.penalty == 'elasticnet':\n",
    "            reg_term = self.C * (self.l1_ratio * np.sum(np.abs(w)) + 0.5 * (1 - self.l1_ratio) * np.sum(w**2))\n",
    "        else:  # none\n",
    "            reg_term = 0\n",
    "        cost = -np.sum(y * np.log(h) + (1 - y) * np.log(1 - h)) / m + reg_term\n",
    "        return cost\n",
    "\n",
    "    def gradient(self, w, X, y):\n",
    "        m = len(y)\n",
    "        h = self.sigmoid(np.dot(X, w))\n",
    "        gradient = np.dot(X.T, (h - y)) / m\n",
    "\n",
    "        if self.penalty == 'l1':\n",
    "            reg_term = self.C * np.sign(w)\n",
    "        elif self.penalty == 'l2':\n",
    "            reg_term = self.C * w\n",
    "        elif self.penalty == 'elasticnet':\n",
    "            reg_term = self.C * (self.l1_ratio * np.sign(w) + (1 - self.l1_ratio) * w)\n",
    "        else:  # none\n",
    "            reg_term = 0\n",
    "        return gradient + reg_term\n",
    "\n",
    "    def gradient_descent(self, X, y, w):\n",
    "        for _ in range(self.max_iter):\n",
    "            gradient = self.gradient(w, X, y)\n",
    "            w -= self.learning_rate * gradient\n",
    "        return w\n",
    "\n",
    "    def sag(self, X, y, w):\n",
    "        m, n = X.shape\n",
    "        sum_gradients = np.zeros_like(w)\n",
    "        gradients = np.zeros((m, n))\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            for i in range(m):\n",
    "                old_gradient = gradients[i]\n",
    "                new_gradient = (self.sigmoid(np.dot(X[i], w)) - y[i]) * X[i]\n",
    "                if self.penalty == 'l1':\n",
    "                    reg_term = self.C * np.sign(w)\n",
    "                elif self.penalty == 'l2':\n",
    "                    reg_term = self.C * w\n",
    "                elif self.penalty == 'elasticnet':\n",
    "                    reg_term = self.C * (self.l1_ratio * np.sign(w) + (1 - self.l1_ratio) * w)\n",
    "                else:  # none\n",
    "                    reg_term = 0\n",
    "                new_gradient += reg_term\n",
    "                gradients[i] = new_gradient\n",
    "                sum_gradients += new_gradient - old_gradient\n",
    "                w -= self.learning_rate * sum_gradients / m\n",
    "        return w\n",
    "    \n",
    "    def hessian(self, w, X, y):\n",
    "        h = self.sigmoid(np.dot(X, w))\n",
    "        hessian = np.dot(((h * (1 - h)).reshape(-1, 1) * X).T, X)\n",
    "\n",
    "        if self.penalty == 'l1':\n",
    "            reg_term = self.C * np.diag(np.sign(w))\n",
    "        elif self.penalty == 'l2':\n",
    "            reg_term = self.C * np.diag(w)\n",
    "        elif self.penalty == 'elasticnet':\n",
    "            reg_term = self.C * np.diag(self.l1_ratio * np.sign(w) + (1 - self.l1_ratio) * w)\n",
    "        else:  # none\n",
    "            reg_term = 0\n",
    "        return hessian + reg_term\n",
    "    \n",
    "    \n",
    "    def newton_cg(self, X, y, w):\n",
    "        for _ in range(self.max_iter):\n",
    "            gradient = self.gradient(w, X, y)\n",
    "            hessian = self.hessian(w, X, y)\n",
    "            step_direction = np.linalg.solve(hessian, gradient)\n",
    "            w -= self.learning_rate * step_direction\n",
    "        return w\n",
    "    \n",
    "    def lbfgs(self, X, y, w_init):\n",
    "        result = minimize(\n",
    "        fun=self._cost_function,\n",
    "        x0=w_init,\n",
    "        args=(X, y),\n",
    "        method='L-BFGS-B',\n",
    "        jac=self._gradient,\n",
    "        tol=self.tol,\n",
    "        options={'maxiter': self.max_iter},\n",
    "    )\n",
    "        return result.x\n",
    "\n",
    "\n",
    "    def saga(self, X, y, w):\n",
    "        m, n = X.shape\n",
    "        sum_gradients = np.zeros_like(w)\n",
    "        gradients = np.zeros((m, n))\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            for i in range(m):\n",
    "                old_gradient = gradients[i]\n",
    "                new_gradient = (self.sigmoid(np.dot(X[i], w)) - y[i]) * X[i]\n",
    "                if self.penalty == 'l1':\n",
    "                    reg_term = self.C * np.sign(w)\n",
    "                elif self.penalty == 'l2':\n",
    "                    reg_term = self.C * w\n",
    "                elif self.penalty == 'elasticnet':\n",
    "                    reg_term = self.C * (self.l1_ratio * np.sign(w) + (1 - self.l1_ratio) * w)\n",
    "                else:  # none\n",
    "                    reg_term = 0\n",
    "                new_gradient += reg_term\n",
    "                gradients[i] = new_gradient\n",
    "                sum_gradients += new_gradient - old_gradient\n",
    "                w -= self.learning_rate * sum_gradients / m\n",
    "            if self.penalty == 'l1':\n",
    "                w = np.sign(w) * np.maximum(np.abs(w) - self.C * self.learning_rate, 0)\n",
    "        return w\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept(X)\n",
    "        w_init = np.zeros(X.shape[1])\n",
    "\n",
    "        if self.solver == 'gradient_descent':\n",
    "            self.w_ = self.gradient_descent(X, y, w_init)\n",
    "        elif self.solver == 'sag':\n",
    "            self.w_ = self.sag(X, y, w_init)\n",
    "        elif self.solver == 'newton_cg':\n",
    "            self.w_ = self.newton_cg(X, y, w_init)\n",
    "        elif self.solver == 'lbfgs':\n",
    "            self.w_ = self.lbfgs(X, y, w_init)\n",
    "        elif self.solver == 'saga':\n",
    "            self.w_ = self.saga(X, y, w_init)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Solver '{self.solver}' not supported. Use 'gradient_descent', 'sag', 'newton_cg', 'lbfgs', or 'saga'.\"\n",
    "            )\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept(X)\n",
    "        return self.sigmoid(np.dot(X, self.w_))\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return (proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5136186770428015\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/5x12/ml-cookbook/master/supplements/data/heart.csv')\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, #independent variables\n",
    "                                                    y, #dependent variable\n",
    "                                                    random_state = 3\n",
    "                                                   )\n",
    "\n",
    "logreg = CustomLogisticRegression(solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8287937743190662\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(solver='lbfgs')\n",
    "logistic.fit(X_train, y_train)\n",
    "y_pred = logistic.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
